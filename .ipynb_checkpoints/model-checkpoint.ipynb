{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import sys\n",
    "import spacy\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "learning_rate = 0.005\n",
    "mesh_dimension=100\n",
    "cnn_dimension=100\n",
    "max_epoch = 100\n",
    "negative_sampling=500\n",
    "num_prediction=15\n",
    "dropout_rate = 0.5\n",
    "early_stop_count = 3\n",
    "\n",
    "def breakparagraph(paragraph):\n",
    "    return sent_tokenize(paragraph)\n",
    "\n",
    "# Loads all of the mesh words\n",
    "file = open(\"mesh_2018_ID.txt\",\"r\")\n",
    "mesh = file.readlines()\n",
    "meshWords = []\n",
    "meshID = []\n",
    "for n, line  in enumerate(mesh):\n",
    "    lineArr = line.split(\"=\")\n",
    "    word = lineArr[0].lower()\n",
    "    meshWords.append(word)\n",
    "\n",
    "# Creates a dictionary to map words to index\n",
    "wordToIdx = {}\n",
    "for n, word in enumerate(meshWords):\n",
    "    wordToIdx[word] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading in Glove\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "vectors=[]\n",
    "\n",
    "# Must download a file into directory for Glove 100d fro this url https://www.kaggle.com/terenceliu4444/glove6b100dtxt\n",
    "# Must rename it to the file name 6B100d.txt\n",
    "with open(\"6B100d.txt\", 'r',encoding= 'utf8') as f:\n",
    "    for l in f:\n",
    "        line = l.split()\n",
    "        word = line[0]\n",
    "\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        vectors.append(vect)\n",
    "\n",
    "glove = {w: vectors[word2idx[w]] for w in words}\n",
    "print('Done loading in Glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#Loading in all of the training and testing data\n",
    "testing_articles=[]\n",
    "test_mesh=[]\n",
    "articles=[]\n",
    "groundtruth_mesh=[]\n",
    "with open('data.json') as json_file:\n",
    "    data= json.load(json_file)\n",
    "    for d in data['train']:\n",
    "        articles.append(d['article'])\n",
    "        groundtruth_mesh.append(d['mesh_labels'])\n",
    "    for d in data['test']:\n",
    "        testing_articles.append(d['article'])\n",
    "        test_mesh.append(d['mesh_labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in all of the candidates for each article in training\n",
    "\n",
    "with open('candidate_data.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    candidate_train = [set(d) for d in data['train']]\n",
    "    sys.stdout.flush()\n",
    "    candidate_test = [set(d) for d in data['test']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mesh label size: 28956\n",
      "28952\n"
     ]
    }
   ],
   "source": [
    "matrix_len = len(meshWords)\n",
    "print('mesh label size:', matrix_len)\n",
    "weights_matrix = np.zeros((matrix_len+1, mesh_dimension))\n",
    "words_found = 0\n",
    "meshWord2Idx = {}\n",
    "\n",
    "# Creates the word embedding matrix for the MEsH labels \n",
    "for i, word in enumerate(meshWords):\n",
    "    meshWord2Idx[word] = i\n",
    "\n",
    "    weights_matrix[i] = np.random.normal(scale=0.6, size=(mesh_dimension, ))\n",
    "#For unknown Mesh Words\n",
    "meshWord2Idx['unk']=len(meshWords)\n",
    "print(len(meshWord2Idx))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "def buildVocab(articles):\n",
    "    vocabulary=set()\n",
    "    for article in articles:\n",
    "        vocabulary.update(set(word_tokenize(article)))\n",
    "    words_found=0\n",
    "    vocab2Index = {}\n",
    "    weights = np.zeros((len(vocabulary)+2, 100))\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        vocab2Index[word] = i\n",
    "        try: \n",
    "            weights[i] = glove[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "\n",
    "            weights[i] = np.random.normal(scale=0.6, size=(100, ))\n",
    "    vocab2Index['unk']=len(vocabulary)\n",
    "    weights[len(vocabulary)] = np.random.normal(scale=0.6, size=(100, ))\n",
    "    vocab2Index['<pad>'] = len(vocabulary)+1\n",
    "    weights[len(vocabulary)+1] = np.random.normal(scale=0.6, size=(100, ))\n",
    "#     print('Glove Embeddings found for Vocab: ' + str(words_found) +' out of ' + str(len(vocabulary)))\n",
    "    return weights, vocab2Index\n",
    "\n",
    "\n",
    "vocab_weights,vocab2Index = buildVocab(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that outputs targets for hingeloss\n",
    "# Input: list of candidate entries , list of groundtruth labels\n",
    "# Output: List of 1 and -1, 1 if is groundtruth label and -1 if not\n",
    "def labelFinder(candEnt, truthEnt):\n",
    "    labels = []\n",
    "    for ent in candEnt:\n",
    "        if candEnt in truthEnt:\n",
    "            labels.append[1]\n",
    "        else:\n",
    "            labels.append[-1]\n",
    "    return labels\n",
    "\n",
    "def meshGenerator(mesh_list):\n",
    "    list_index = []\n",
    "    for mesh in mesh_list:\n",
    "        mesh = mesh.lower()\n",
    "        if mesh in meshWord2Idx:\n",
    "            list_index.append(meshWord2Idx[mesh])\n",
    "        else:\n",
    "            list_index.append(meshWord2Idx['unk'])\n",
    "    return torch.LongTensor(list_index)\n",
    "\n",
    "\n",
    "def helper(groundtruth_mesh, candid_mesh, train=False):\n",
    "    pos_positions = []\n",
    "    neg_positions = []\n",
    "    for n,mesh in enumerate(candid_mesh):\n",
    "        \n",
    "        if mesh in groundtruth_mesh:\n",
    "            pos_positions.append(n)\n",
    "        else:\n",
    "            neg_positions.append(n)\n",
    "    if pos_positions==[]:\n",
    "        return [],[],[]\n",
    "\n",
    "    pos_labels_ = np.random.choice(pos_positions, size=500)\n",
    "\n",
    "    neg_labels_ = np.random.choice(neg_positions, size=500)\n",
    "    pos_list = torch.LongTensor(pos_labels_)\n",
    "    neg_list = torch.LongTensor(neg_labels_)\n",
    "    return pos_list, neg_list, pos_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "class MeshNN(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers):\n",
    "        super(MeshNN,self).__init__()\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return self.embedding(inp)\n",
    "    \n",
    "\n",
    "    \n",
    "# Constructor takes in the weight matrix for the vocabulary\n",
    "# Input: One sentence\n",
    "# Output: Sentence embedding\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,weights_matrix):\n",
    "        super(CNN,self).__init__()\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "        self.conv0= nn.Conv2d(in_channels = 1,\n",
    "                             out_channels = int(cnn_dimension / 2),\n",
    "                             kernel_size = (3,100))\n",
    "        self.conv1= nn.Conv2d(in_channels = 1,\n",
    "                             out_channels = int(cnn_dimension / 2),\n",
    "                             kernel_size = (4,100))\n",
    "        self.linear = nn.Linear(int(cnn_dimension/2) * 2, mesh_dimension)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "\n",
    "        c0 = F.relu(self.conv0(embedded).squeeze(3))\n",
    "        c1 = F.relu(self.conv1(embedded).squeeze(3))\n",
    "        #print(c0.size(), c1.size())\n",
    "        pool0 = F.max_pool1d(c0, c0.shape[2]).squeeze(2)\n",
    "        pool1 = F.max_pool1d(c1, c1.shape[2]).squeeze(2) \n",
    "        #print(pool0.size(), pool1.size())\n",
    "        #sys.stdout.flush()\n",
    "        cat = self.dropout(torch.cat((pool0, pool1), dim = 1))\n",
    "        return self.linear(cat)\n",
    "\n",
    "# EnsembleNN Model that takes in two models and a weight matrix: one model to map GLOVe words and another model to encode sentences\n",
    "# The weight matrix holds the word embeddings for all of the mesh words\n",
    "\n",
    "class EnsembleNN(nn.Module):\n",
    "    def __init__(self,MeshModel, sent_embedding_model,weightMatrix,vocabMatrix):\n",
    "        super(EnsembleNN,self).__init__()\n",
    "        self.meshModel = MeshModel(weightMatrix,1,1)\n",
    "        self.sentModel = sent_embedding_model(vocabMatrix)\n",
    "        # self.lin1 = nn.Linear(768,dimension)\n",
    "\n",
    "        # forward function takes in a list of sentences and a Long Tensor of indices of mesh labels\n",
    "    def forward(self, sentences, meshLabels):\n",
    "        colMatrix = self.meshModel(meshLabels) # num_candidate, mesh_dim\n",
    "        rowMatrix = self.sentModel(sentences) # num_entity, mesh_dim\n",
    "        # rowMatrix = self.lin1(torch.FloatTensor(rowMatrix))\n",
    "        fullMatrix = rowMatrix.mm(colMatrix.t()) # num_entity, num_candidate\n",
    "        #meanMatrix = torch.logsumexp(fullMatrix,dim=0) \n",
    "        meanMatrix = torch.max(fullMatrix,dim=0)[0]\n",
    "        return meanMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_tp_fp(pred_mesh, groundtruth_mesh):\n",
    "    pred_mesh_set = set(pred_mesh)\n",
    "    groundtruth_set = groundtruth_mesh\n",
    "    true_positives = pred_mesh_set.intersection(groundtruth_set)\n",
    "    length_false_positives = len(pred_mesh_set) - len(true_positives)\n",
    "    return len(true_positives),length_false_positives\n",
    "        \n",
    "\n",
    "def building_right_wrong_array(pred_mesh, groundtruth_mesh, arr):\n",
    "    for mesh in pred_mesh:\n",
    "        if mesh in groundtruth_mesh:\n",
    "            arr.append(1)\n",
    "        else:\n",
    "            arr.append(0)\n",
    "\n",
    "def convert_text_to_id(articles):\n",
    "    \n",
    "    articles_wid = []\n",
    "    for n,article in enumerate(articles):\n",
    "        sentences = breakparagraph(article)\n",
    "        # Add <start> and <end> tags around the entities in the sentences\n",
    "        tokenized_sentences = []\n",
    "        for sentence in sentences:\n",
    "            doc = nlp(sentence)\n",
    "            entities_test = list(doc.ents)\n",
    "            for ent in entities_test:\n",
    "                idx = sentence.index(str(ent))\n",
    "                new_string = sentence[:idx] + ' <start> ' + sentence[idx: idx+len(str(ent))] + ' <end> '+ sentence[idx+len(str(ent)):]\n",
    "                tokenized_sentences.append(new_string)\n",
    "\n",
    "        # Tokenize the words in the list of sentences\n",
    "        word_sentences = []\n",
    "        max_length = 0\n",
    "        for sentence in tokenized_sentences:\n",
    "            toAdd = word_tokenize(sentence)\n",
    "            word_sentences.append(word_tokenize(sentence))\n",
    "            if max_length < len(toAdd):\n",
    "                max_length = len(toAdd)\n",
    "\n",
    "        # Pads sentences for those that are not of max length\n",
    "        for sentence in word_sentences:\n",
    "            if len(sentence)<max_length:\n",
    "                while len(sentence)!=max_length:\n",
    "                    sentence.append('<pad>')\n",
    "\n",
    "        batch_sentences = []\n",
    "        index_text = []\n",
    "        for s in word_sentences:\n",
    "            index_text = []\n",
    "            for t in s:\n",
    "                if t in vocab2Index:\n",
    "                    index_text.append(vocab2Index[t])\n",
    "                else:\n",
    "                    index_text.append(vocab2Index['unk'])\n",
    "            batch_sentences.append(index_text)\n",
    "        sentences_id = torch.LongTensor(batch_sentences)\n",
    "        articles_wid.append(sentences_id)\n",
    "    return articles_wid\n",
    "\n",
    "training_articles_wid = convert_text_to_id(articles)\n",
    "testing_articles_wid = convert_text_to_id(testing_articles)\n",
    "\n",
    "def evaluate():\n",
    "    losses = []\n",
    "    test_articles = testing_articles_wid\n",
    "    test_meshlabels = test_mesh\n",
    "    tp=0\n",
    "    fp=0\n",
    "    total_mesh=0\n",
    "    with torch.no_grad():\n",
    "        for n,article in enumerate(test_articles): \n",
    "            setHold = meshGenerator(candidate_test[n])\n",
    "\n",
    "            mesh_list = meshGenerator(test_meshlabels[n])\n",
    "            out = model(article, setHold)\n",
    "#             out = out[0]\n",
    "            values, indices = out.topk(num_prediction)\n",
    "            pred_mesh = [meshWords[n]for n in setHold[indices]]\n",
    "            mesh_listing = set([word.lower() for word in test_meshlabels[n]])\n",
    "            pos_list,neg_list,pos_positions = helper(mesh_list,setHold)\n",
    "#             if n ==5:\n",
    "#                 print('Values of true mesh labels',out[pos_positions])\n",
    "            t,f = calculate_tp_fp(pred_mesh,mesh_listing)\n",
    "            tp+=t\n",
    "            fp+=f\n",
    "            total_mesh+=len(mesh_listing)\n",
    "            \n",
    "            if len(pos_list)==0:\n",
    "                continue\n",
    "            scores = out[pos_list] - out[neg_list]\n",
    "            loss = criterion(scores, torch.tensor([-1]*negative_sampling))\n",
    "            losses.append(loss)\n",
    "\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/total_mesh\n",
    "    print('Average Testing Loss',sum(losses)/len(losses))\n",
    "    if precision+recall==0:\n",
    "        return 0,0,0\n",
    "    else:\n",
    "        \n",
    "        micro_score = 2*precision*recall/(precision+recall)\n",
    "        return micro_score,recall,precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "sys.stdout.flush()\n",
    "vocab_weights = torch.FloatTensor(vocab_weights)\n",
    "torch_weights = torch.FloatTensor(weights_matrix)\n",
    "# labelIndexes = torch.LongTensor(labelIndexes)\n",
    "cnn_model = CNN(vocab_weights)\n",
    "model = EnsembleNN(MeshNN, CNN, torch_weights, vocab_weights)\n",
    "# Creating optimizer and loss functions\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.HingeEmbeddingLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "Average Testing Loss tensor(0.4913)\n",
      "Testing Micro score: 0.06060606060606061 | Recall 0.08641975308641975 | Precision 0.04666666666666667\n",
      "Average Training Loss tensor(0.6548, grad_fn=<DivBackward0>)\n",
      "Training set micro score 0.10793650793650794 | Recall 0.09516539440203563 | Precision 0.12466666666666666\n",
      "GROUNDTRUTH f-score 0.7451659451659453 | Recall 0.656997455470738 | Precision 0.8606666666666667\n",
      "------------------End of Epoch 0 -------------------\n",
      "0\n",
      "10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-f4ee34c896af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mpos_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mneg_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos_positions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroundtruth_mesh_indices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcandidate_mesh_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-13ac3d66be2c>\u001b[0m in \u001b[0;36mhelper\u001b[1;34m(groundtruth_mesh, candid_mesh, train)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmesh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandid_mesh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mmesh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgroundtruth_mesh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mpos_positions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(i)\u001b[0m\n\u001b[0;32m    426\u001b[0m                           \u001b[1;34m'iterations executed (and might lead to errors or silently give '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m                           'incorrect results).', category=RuntimeWarning)\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training cell\n",
    "n=0\n",
    "\n",
    "prev_macro = None\n",
    "prev_micro=None\n",
    "early_stop=False\n",
    "for e in range(max_epoch):\n",
    "    tp=0\n",
    "    fp=0\n",
    "    total_mesh = 0\n",
    "    losses=[]\n",
    "    g_tp=0\n",
    "    g_fp=0\n",
    "    for n,article in enumerate(training_articles_wid): \n",
    "\n",
    "        if n%10==0:\n",
    "            print(n)\n",
    "\n",
    "        candidate_mesh_indices = meshGenerator(candidate_train[n])\n",
    "\n",
    "        groundtruth_mesh_indices = meshGenerator(groundtruth_mesh[n])\n",
    "\n",
    "\n",
    "        pos_list,neg_list,pos_positions = helper(groundtruth_mesh_indices,candidate_mesh_indices)\n",
    "        if len(pos_list)==0:\n",
    "            continue\n",
    "\n",
    "        out = model(article, candidate_mesh_indices)\n",
    "\n",
    "        scores = out[pos_list] - out[neg_list]\n",
    "        loss = criterion(scores, torch.tensor([-1]*negative_sampling))\n",
    "        losses.append(loss)\n",
    "        values, indices = out.topk(num_prediction)\n",
    "        pred_mesh = [meshWords[n] for n in candidate_mesh_indices[indices]]\n",
    "        mesh_listing = set([word.lower() for word in groundtruth_mesh[n]])        \n",
    "        t,f = calculate_tp_fp(pred_mesh,mesh_listing)\n",
    "        tp+=t\n",
    "        fp+=f\n",
    "        total_mesh+=len(mesh_listing)\n",
    "        \n",
    "        candidate_mesh_set = set(candidate_mesh_indices)\n",
    "\n",
    "        \n",
    "        candidate_mesh_name_set = set(candidate_train[n])\n",
    "        groundtruth_mesh_set = set(mesh_listing)\n",
    "        intersected_set = candidate_mesh_name_set.intersection(groundtruth_mesh_set)\n",
    "\n",
    "        \n",
    "        g_tp += min(len(intersected_set),num_prediction)\n",
    "\n",
    "        g_fp += max(0,num_prediction-min(len(intersected_set),num_prediction))\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        s5=time.time()\n",
    "\n",
    "        if n==99 and n!=0:\n",
    "\n",
    "            micro_score, recall, precision = evaluate()\n",
    "            print('Testing Micro score:',micro_score, '| Recall',recall,'| Precision',precision)\n",
    "            if prev_micro!=None and (micro_score<prev_micro):\n",
    "                print('Micro score:', micro_score,'previous micro score',prev_micro, '| Recall',recall,'| Precision',precision)\n",
    "                count+=1\n",
    "            else:\n",
    "                count =0\n",
    "            if count==early_stop_count:\n",
    "                \n",
    "                early_stop=True\n",
    "                break\n",
    "            prev_micro = micro_score\n",
    "            sys.stdout.flush()\n",
    "        n+=1\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/total_mesh\n",
    "    print('Average Training Loss',sum(losses)/len(losses))\n",
    "\n",
    "    if precision+recall==0:\n",
    "        print('Micro score at 0')\n",
    "    else:\n",
    "        micro_score = 2*precision*recall/(precision+recall)\n",
    "        print('Training set micro score',micro_score, '| Recall',recall,'| Precision',precision)\n",
    "        \n",
    "    ground_recall = g_tp/total_mesh\n",
    "    ground_precision = g_tp/(g_tp+g_fp)\n",
    "    ground_micro = 2*ground_recall*ground_precision/(ground_recall+ground_precision)\n",
    "    print('GROUNDTRUTH f-score',ground_micro,'| Recall',ground_recall,'| Precision',ground_precision)\n",
    "    \n",
    "    \n",
    "    if early_stop:\n",
    "        break\n",
    "    print('------------------End of Epoch',e,'-------------------')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
