{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import fasttext\n",
    "from scipy import spatial\n",
    "from numpy import linalg as LA\n",
    "import json\n",
    "import spacy\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_candid=500\n",
    "candid_gen_type = 'fasttext' # Pick between 'fasttext', 'tf-idf', 'bm25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads all of the mesh words\n",
    "file = open(\"mesh_2018_ID.txt\",\"r\")\n",
    "mesh = file.readlines()\n",
    "meshWords = []\n",
    "meshID = []\n",
    "for n, line  in enumerate(mesh):\n",
    "    lineArr = line.split(\"=\")\n",
    "    word = lineArr[0].lower()\n",
    "    meshWords.append(word)\n",
    "\n",
    "# Creates a dictionary to map words to index\n",
    "wordToIdx = {}\n",
    "for n, word in enumerate(meshWords):\n",
    "    wordToIdx[word] = n\n",
    "\n",
    "#Loading in all of the training and testing data\n",
    "testing_articles=[]\n",
    "test_mesh=[]\n",
    "articles=[]\n",
    "groundtruth_mesh=[]\n",
    "with open('data.json') as json_file:\n",
    "    data= json.load(json_file)\n",
    "    for d in data['train']:\n",
    "        articles.append(d['article'])\n",
    "        groundtruth_mesh.append(d['mesh_labels'])\n",
    "    for d in data['test']:\n",
    "        testing_articles.append(d['article'])\n",
    "        test_mesh.append(d['mesh_labels'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate Generator 1: Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfif_char_vectorizer = TfidfVectorizer(decode_error='ignore', stop_words='english',\n",
    "                                       analyzer='char', ngram_range=(2, 5),\n",
    "                                       max_features=100000)\n",
    "\n",
    "umls_tfidf = tfif_char_vectorizer.fit_transform(meshWords).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "def candidMesh(groundtruth_mesh, article, num_of_candid):\n",
    "    mentions=[]\n",
    "    mentionSet = set()\n",
    "    doc = nlp(article)\n",
    "    for ent in doc.ents:\n",
    "        mentions.append(str(ent))\n",
    "\n",
    "    mention_tfidf = tfif_char_vectorizer.transform(mentions).tocsr()\n",
    "\n",
    "\n",
    "    # try sharding this\n",
    "    shard_start = 0\n",
    "    delta = mention_tfidf.shape[0] \n",
    "    keep_scores = []\n",
    "    topk_hits = defaultdict(int)\n",
    "    topk_misses = defaultdict(int)\n",
    "    for shard_num in range(1):\n",
    "        shard_end = shard_start + delta\n",
    "\n",
    "        pair_scores = pairwise_distances(mention_tfidf[shard_start:shard_end,:], umls_tfidf, metric='cosine', n_jobs=-1)\n",
    "        arg_mins = np.argpartition(pair_scores, num_of_candid, axis=1)\n",
    "        for row in range(pair_scores.shape[0]):\n",
    "            hit = False\n",
    "            cur_mins = arg_mins[row]\n",
    "            for topK in range(num_of_candid):\n",
    "                k = 0\n",
    "                while k < topK+1 and k < arg_mins.shape[1] and not hit:\n",
    "                    match_idx = arg_mins[row, k]\n",
    "                    mentionSet.add(meshWords[match_idx])\n",
    "                    k+=1\n",
    "\n",
    "    count = 0\n",
    "#     for entity in groundtruth_mesh:\n",
    "#         if entity.lower() in mentionSet:\n",
    "#             count+=1\n",
    "#     print(\"RIGHT:\",count,\"OUT OF\", len(groundtruth_mesh))\n",
    "#     print(len(mentionSet))\n",
    "#     averages.append(count/len(groundtruth_mesh))\n",
    "#     groundtruth_mesh = []\n",
    "#     print('Size of Mention set',len(mentionSet))\n",
    "    return mentionSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate Generator 2: BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm_candidate_gen(article, num_of_candid):\n",
    "    corpus = [str(m) for m in meshWords]\n",
    "    bm25 = BM25Okapi(corpus)\n",
    "    mentions=[]\n",
    "    mentionSet = set()\n",
    "    doc = nlp(article)\n",
    "    for ent in doc.ents:\n",
    "        mentions.append(str(ent))\n",
    "    \n",
    "    corpus = [str(m) for m in meshWords]\n",
    "    for word in mentions:\n",
    "    \n",
    "\n",
    "        mentionSet = mentionSet.union(set(bm25.get_top_n(word, corpus, n=num_of_candid)))\n",
    "\n",
    "    return mentionSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate Generator 3: Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_fasttext = fasttext.train_unsupervised('mesh-words.txt', minn=2, maxn=5, dim=300,epoch=25,verbose=2)\n",
    "mesh_vectors_fasttext =[]\n",
    "for word in meshWords:\n",
    "    mesh_vectors_fasttext.append(model_fasttext.get_word_vector(word))\n",
    "tree = spatial.KDTree(mesh_vectors_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_np = np.array(meshWords)\n",
    "def fasttext_candidate_gen(article, num_of_candid):\n",
    "    mentions=[]\n",
    "    mentionSet = set()\n",
    "    doc = nlp(article)\n",
    "    for ent in doc.ents:\n",
    "        mentions.append(str(ent))\n",
    "    word_vect = [model_fasttext.get_word_vector(word_candid) for word_candid in mentions]\n",
    "    \n",
    "\n",
    "    words = mesh_np[tree.query(word_vect,k=num_of_candid)[1]]\n",
    "    word_set = [set(w) for w in words]\n",
    "\n",
    "    mentionSet = mentionSet.union(*word_set)\n",
    "\n",
    "    return mentionSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Candidate Sets from one of the three candidate generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running candidate generation | type: fasttext\n",
      "On training article 0\n",
      "On training article 10\n",
      "On training article 20\n",
      "On training article 30\n",
      "On training article 40\n",
      "On training article 50\n",
      "On training article 60\n",
      "On training article 70\n",
      "On training article 80\n",
      "On training article 90\n"
     ]
    }
   ],
   "source": [
    "# Generating candidates through tf idf\n",
    "candidate_mesh_list_train = []\n",
    "candidate_mesh_list_test = []\n",
    "print('Running candidate generation | type:',candid_gen_type)\n",
    "if candid_gen_type == 'tf-idf':\n",
    "    for n, article in enumerate(articles):\n",
    "        candidate_mesh_list_train.append(candidMesh(groundtruth_mesh[n],article,num_of_candid))\n",
    "    for n, article in enumerate(testing_articles):\n",
    "        candidate_mesh_list_test.append(candidMesh(test_mesh[n],article,num_of_candid))\n",
    "        \n",
    "        \n",
    "elif candid_gen_type == 'bm25':\n",
    "    for n, article in enumerate(articles):\n",
    "        candidate_mesh_list_train.append(bm_candidate_gen(article,num_of_candid))\n",
    "        if n%10==0:\n",
    "            print('On training article',n)\n",
    "    for n, article in enumerate(testing_articles):\n",
    "        candidate_mesh_list_test.append(bm_candidate_gen(article,num_of_candid))\n",
    "        \n",
    "        \n",
    "elif candid_gen_type =='fasttext':\n",
    "    for n, article in enumerate(articles):\n",
    "        candidate_mesh_list_train.append(fasttext_candidate_gen(article,num_of_candid))\n",
    "        if n%10==0:\n",
    "            print('On training article',n)\n",
    "    for n, article in enumerate(testing_articles):\n",
    "        candidate_mesh_list_test.append(fasttext_candidate_gen(article,num_of_candid))\n",
    "        \n",
    "else:\n",
    "    print('Invalid candidate generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "can = [list(s) for s in candidate_mesh_list_train]\n",
    "candidate_mesh_list_train = can\n",
    "can = [list(s) for s in candidate_mesh_list_test]\n",
    "candidate_mesh_list_test = can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={}\n",
    "data['train'] = candidate_mesh_list_train\n",
    "data['test'] = candidate_mesh_list_test\n",
    "found=True\n",
    "if candid_gen_type=='tf-idf':\n",
    "    filename = 'candidate_data.json'\n",
    "elif candid_gen_type=='bm25':\n",
    "    filename = 'candidate_data_bm25.json'\n",
    "elif candid_gen_type =='fasttext':\n",
    "    filename='candidate_data_fasttext.json'\n",
    "else:\n",
    "    found=False\n",
    "if found:\n",
    "    with open(filename,'w') as outfile:\n",
    "        json.dump(data,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
